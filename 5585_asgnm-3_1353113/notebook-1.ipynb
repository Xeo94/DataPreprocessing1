{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f5cf33f760ea3e25530dfa31c5e05d16",
     "grade": false,
     "grade_id": "cell-ca69b12bc0f90bc0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Assignment 3\n",
    "\n",
    "Please complete the following three tasks on the dataset assigned to you. Inspect the dataset visually and using proper Python instrumentation, and establish whether:\n",
    "\n",
    "* the dataset is tidy? If yes, demonstrate (Python code) and describe why this is the case. If no, reshape (Python code) the dataset into a tidy one and describe the steps performed.\n",
    "* the dataset includes missing data? If no, demonstrate (Python code) and describe why this conclusion is warranted. If yes, apply a handling strategy for missing data (Python code) and discuss the consequences.\n",
    "* the dataset includes duplicate data? If no, demonstrate (Python code) and describe why this conclusion is justified. If yes, deduplicate the data set (Python code) and describe the steps performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2b0d2e12b1836846f17271ba13e95777",
     "grade": false,
     "grade_id": "cell-3f5b113d86d67af9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Task 1 (8 credits): Tidy vs. messy?\n",
    "\n",
    "* Is the dataset tidy according to the three rules covered in Unit 3?\n",
    "* If yes, show and discuss why?\n",
    "* If no, tidy the dataset and document the steps performed?\n",
    "\n",
    "In addition, characterize the final, tidy dataset:\n",
    "* What are the data objects? How many different kinds of data objects are covered?\n",
    "* What are the different variables? Describe each variable along the dimensions covered in Unit 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "023573cf0134ba652d91d7bcee887b95",
     "grade": true,
     "grade_id": "cell-43955a752e160df6",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#importing pandas\n",
    "import pandas as pd\n",
    "#opening the main file in pandas\n",
    "edstat=pd.read_csv(\"./data/data_notebook-1_EdStatsData.csv\")\n",
    "#Showing the column names and values in them:\n",
    "print(\"--------------------------------------------\")\n",
    "print(\"Displaying the data from data_notebook-1_EdStatsData.csv: \")\n",
    "print(\"--------------------------------------------\")\n",
    "print(edstat.info())\n",
    "print(\"--------------------------------------------\")\n",
    "#Displaying the first 10 rows\n",
    "print(\"Displaying first 10 rows: \")\n",
    "print(\"--------------------------------------------\")\n",
    "print(edstat.head(10))\n",
    "print(\"--------------------------------------------\")\n",
    "#Displaying last 10 rows\n",
    "print(\"Displaying last 10 rows: \")\n",
    "print(\"--------------------------------------------\")\n",
    "print(edstat.tail(10))\n",
    "print(\"--------------------------------------------\")\n",
    "#isolating the valid columns (columns which represent valid columns (they form variables), first 4 in this case)\n",
    "ids=list(edstat.columns[:4])\n",
    "#Transforming the table by adding a column with values from a certain year using pd.melt() function\n",
    "# information about this function I found here: https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.melt.html\n",
    "print(\"Transforming the dataset...\")\n",
    "df1=pd.melt(edstat,id_vars=ids,var_name=\"Year\",value_name=\"Value\",col_level=0)\n",
    "print(\"Transformation complete!\")\n",
    "print(\"--------------------------------------------\")\n",
    "print(\"Inspecting the transformed dataset:\")\n",
    "print(\"--------------------------------------------\")\n",
    "print(df1.head(10))\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "752e91c2755208b850f74e8fcec39896",
     "grade": true,
     "grade_id": "cell-13d5d5fb5deac694",
     "locked": false,
     "points": 3,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "I have opted for the Option 2 because data sets I used in assignment 2 were tidy and there was not so much work to be done on them.\n",
    "This dataset is downloaded from the World Bank data portal (https://datacatalog.worldbank.org/dataset/education-statistics)\n",
    "And cointains Education Statistics data from around the world.\n",
    "Some of the csv files are supporting files, they explain the meaning of certain indicators and meaning of indicator codes.\n",
    "The main dataset is: data_notebook-1_EdStatsData.csv\n",
    "I have inspected all the datasets and found the \"main\" data set using external (not python) software like SublimeText and Pages. \n",
    "Supplemental datasets go as follows: \n",
    "- data_notebook-1_EdStatsCountry-Series.csv ---> Contains explanatin for different series codes when it comes to estimates\n",
    "- data_notebook-1_EdStatsCountry.csv ----> Cointains detailed country information\n",
    "- data_notebook-1_EdStatsFootNote.csv ----> Contains country code, series code year and description \n",
    "- data_notebook-1_EdStatsSeries.csv ---> Contains series codes and indicator explanations\n",
    "\n",
    "Original data set has 70 columns first 4 cover the variables Country Name, Country Code, Indicator Name, Indicator Code, the remaining 66 columns represent the years and measurements for those years (1970-2017 is historic data and from 2017-2100 is future projected data). This dataset can be caracterized as \"messy\".\n",
    "In order to make this data set tidy each variable should form a column, that means that year and value column need to be introduced. I did this with help of pandas function .melt() which transformed the table.\n",
    "(https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.melt.html)\n",
    "This new table has following columns: (index), Country(Categorical data), Name(Categorical data), Country Code(Categorical), Indicator Name(Categorical), Indicator Code(Categorical), Year(Categorical), Value(Numerical).\n",
    "Now the dataset has observation listed in rows and variables as columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8cda404effbb4976f9cce170b8302cba",
     "grade": false,
     "grade_id": "cell-486252309b906a40",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Task 2 (4 credits): Missing data?\n",
    "\n",
    "* Does the dataset contain missing data?\n",
    "* If no, show and give proof that this is actually the case.\n",
    "* If yes, extract the missings and describe them.\n",
    "* If yes, apply a (simple) handling strategy and briefly discuss the implications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "20099135fdaaedf0ad3867f176c329c6",
     "grade": true,
     "grade_id": "cell-016f238ab95f20a7",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#Number of missings per column:\n",
    "print(\"--------------------------------------------\")\n",
    "print(\"Showing the observations with missing values in column Value:\")\n",
    "print(df1.loc[df1[\"Value\"].isna()].head(10))\n",
    "print(\"--------------------------------------------\")\n",
    "print(\"Overview of the columns and missing data (NaN's) in them: \")\n",
    "print(df1.isna().sum())\n",
    "length_before_delete=len(df1) #ammount of rows before cleanup\n",
    "print(\"Deleting the NaN rows...\")\n",
    "df1.dropna(axis=0,inplace=True)#dropping the NaN's\n",
    "percentage=round((len(df1)/length_before_delete)*100,2) # Calculate percentage of deleted data and round it on 2nd decimal point\n",
    "print(\"--------------------------------------------\")\n",
    "print(\"The percentage of deleted rows is: \" +str(percentage)+\" %\")\n",
    "print(\"--------------------------------------------\")\n",
    "\n",
    "print(\"--------------------------------------------\")\n",
    "print(\"Inspecting the data after deleting the missing data (NaN's)\")\n",
    "print(df1.head(10))\n",
    "print(df1.tail(10))\n",
    "print(\"--------------------------------------------\")\n",
    "print(\"Overview of the columns and ammount of missings after the cleaning: \")\n",
    "print(df1.isna().sum())\n",
    "\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "05f6c1aa281b7b781a58bac66a86c6ef",
     "grade": true,
     "grade_id": "cell-b5f58e0a1acc2fdc",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "We can see that a lot of data is missing. Since these observations are all\n",
    "coming from many countries and these countries can have different measurement\n",
    "standards, different education standards and different burocratic arragements\n",
    "I can't say that data is MCAR because of the reasons above (missingness might be dependent on the country or region)\n",
    "I will assume that the data is MAR because the misingness can be explained by another variable (country or region in this case).\n",
    "In the end I decided to delete missing data (NaN's) and approximately 9% of data has been deleted, which can introduce bias into the statistical analysis. This, however depends on the research question and the goal of analysis. \n",
    "In this task I used pandas because NaN has different formats and because we used pandas to find missings in notebooks we covered in class (https://nbviewer.jupyter.org/urls/datascience.ai.wu.ac.at/ss19/dataprocessing1/notebooks/missings-dupes.ipynb).\n",
    "At the end we can see that all NaN values are deleted from the \"Value\" column. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "70ceda154d9bbf95be956485f5b088c8",
     "grade": false,
     "grade_id": "cell-b3f686df5b667b2d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Task 3 (3 credits): Duplicate data?\n",
    "\n",
    "* Does the dataset contain duplicates?\n",
    "* If no, show and give proof that this is actually the case.\n",
    "* If yes, extract the duplicates and describe them.\n",
    "* If yes, make an attempt to remove duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "2a3ec65e292c6f8f9507a38f04f37a06",
     "grade": true,
     "grade_id": "cell-83b42493e050fb7f",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "duplicates=[df1.duplicated(keep=False)]\n",
    "df1.reset_index(inplace=True)\n",
    "#Sometimes the final dataframe would have 2 index columns, I tried to solve that in the next 4 lines by dropping the \n",
    "# column \"index\"\n",
    "try:\n",
    "    df1.drop(columns=[\"index\"],inplace=True)\n",
    "except:\n",
    "    print(\"There is no column named 'index'.\" )\n",
    "print(df1.head(100))\n",
    "print(df1.tail(100))\n",
    "\n",
    "print(\"Showing info from the finished dataset: \")\n",
    "df1.info()\n",
    "\n",
    "#Outputing the finished dataset:\n",
    "print(\"Creating file data_notebook-1_TidyEdstats.csv ...\")\n",
    "df1.to_csv(\"./data/data_notebook-1_TidyEdstats.csv\",index=False)\n",
    "print(\"Creation complete!\")\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ec4f14b7032a3d573edbc6996cbb5193",
     "grade": true,
     "grade_id": "cell-ab56ddc1e92ffc8f",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "Discussing duplicate data:\n",
    "As a result of \"pd.df.melt()\" I reduced a number of columns by storing year variables in \"Year\" column and their values in \"Value\" column. This has resulted in a \"longer\" dataset. As a result of the \"melt\", there is no duplicate values. Only thing I have to check is if there are duplicates caused by human or software error. This I have done using pandas .duplicated() function and it has not returned any duplicates. \n",
    "This brings me to the end of this assignment. The final dataset is exported in \"./data\" directory  as \"data_notebook-1_TidyEdstats.csv\". I did not overwrite the original csv file, instead I chose to keep the original data set and export the \"tidy\" data set seperatly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b1c611f616b2294c511fa0bb261cdd7e",
     "grade": false,
     "grade_id": "cell-51f90e58a3f7af4a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Bonus task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "08c42fd4dae475c545061b5bf71b795f",
     "grade": true,
     "grade_id": "cell-03f0830938a7898a",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#raise NotImplementedError()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
